}\documentclass[12pt]{article}
\usepackage{amsmath, amssymb}
\usepackage{geometry}
\usepackage{enumitem}
\usepackage{listings}
\usepackage{tcolorbox}
\tcbuselibrary{skins}

\geometry{letterpaper, margin=1in}

\usepackage{minted}

\setlength{\parindent}{0pt}
\setlength{\parskip}{0.6em}

\lstset{
    basicstyle=\ttfamily\small,
    frame=single,
    numbers=none,
    tabsize=4,
    showstringspaces=false,
    breaklines=true
}

\begin{document}

\section*{Worked Solutions}


\begin{enumerate}[label=\arabic*.]

\item \textbf{What happens?} We go through the array from the first element to the last and add each number to a running total.

\textbf{Why that matters:} The loop touches each element exactly once. If there are $n$ elements, there are $n$ additions.

\textbf{Small example:} If the array has 5 numbers, you do 5 additions.

\textbf{Final answer:} The time grows roughly in proportion to $n$. Write it as $\boxed{O(n)}$.

\hrulefill

\item \textbf{What happens?} There are two loops: for each value of \texttt{i} (1 to $n$), you run the inner loop that does \texttt{n} prints. So for \texttt{i}=1 you do $n$ prints, for \texttt{i}=2 you do $n$ prints, and so on.

\textbf{Counting:} How many prints total? It's $n$ (from the outer loop) times $n$ (from the inner loop) = $n \times n = n^2$ prints.

\textbf{Small example:} If $n=3$, prints are: (i=1 $\rightarrow$ 3 prints), (i=2 $\rightarrow$ 3 prints), (i=3 $\rightarrow$ 3 prints). Total = 9 = $3^2$.

\textbf{Final answer:} The running time grows like the square of $n$. Write it as $\boxed{O(n^2)}$.

\hrulefill

\item \textbf{What happens?} You start at 1, then multiply by 3 each time: values go 1, 3, 9, 27, etc., until you pass $n$.

\textbf{Why that matters:} Each step makes the value much bigger (triples it), so the number of steps is small compared to $n$. The number of times you can triple 1 before exceeding $n$ is about how many times you can double or triple, i.e., it grows like a logarithm.

\textbf{Small example:} If $n=27$, sequence is 1,3,9,27 — that's 4 steps (still small). If $n=81$, sequence is 1,3,9,27,81 — 5 steps.

\textbf{Final answer:} The loop runs about logarithmically many times. Write it as $\boxed{O(\log n)}$. (Base of log does not matter for big-picture.)

\hrulefill

\item \textbf{What happens?} To test if $n$ is prime in this simple method, you try dividing $n$ by every whole number from 2 up to the square root of $n$.

\textbf{Why square root?} If $n$ has a factor bigger than $\sqrt{n}$, it must pair with a factor smaller than $\sqrt{n}$. So checking up to $\sqrt{n}$ is enough.

\textbf{Small example:} If $n=49$, $\sqrt{n}=7$, check divisors 2..7. If none divide, 49 is prime (in this case 7 does, so it's not prime).

\textbf{Final answer:} You do about $\sqrt{n}$ divisions, so the time is $\boxed{O(\sqrt{n})}$.

\hrulefill

\item \textbf{What happens?} For each number $i$ from 1 to $n$, you set $j=i$ and then repeatedly divide $j$ by 2 until $j$ becomes 0. That inner process runs faster when $i$ is small and a little longer when $i$ is large.

\textbf{How long for one $i$?} If $j$ starts at $i$, dividing by 2 repeatedly takes roughly how many times you can halve $i$ until it reaches 0. That number is about log base 2 of $i$. For example, if $i=8$ then sequence 8,4,2,1,0 — about 4 halvings.

\textbf{Add them all up:} Now add the time for $i=1$, $i=2$, ..., $i=n$. That is:
\[
\text{time} \approx \log 1 + \log 2 + \log 3 + \dots + \log n.
\]
Intuitively, that sum is roughly $n$ times the “average log,” which ends up being proportional to $n \times \log n$.

\textbf{Small illustration for $n=8$:}
\begin{itemize}
    \item i=1: inner steps = 0 or 1
    \item i=2: inner steps = 1
    \item i=3: inner steps = 2
    \item i=4: inner steps = 2
    \item i=5..8: inner steps = 3
\end{itemize}
Adding these gives a number that grows roughly like $n \log n$ when $n$ gets large.

\textbf{Final answer:} $\boxed{O(n \log n)}$.

\hrulefill

\item \textbf{What happens?} You have $m$ separate queries. Each query takes time proportional to $\log n$.

\textbf{Combine them:} Doing $m$ queries means you do $\log n$ work $m$ times, so total is $m \times \log n$.

\textbf{Final answer:} $\boxed{O(m \log n)}$.

\hrulefill

\item \textbf{What happens?} The outer loop runs $i$ from 1 up to $n$. For each $i$, the inner loop prints $i$ times (from 1 to $i$).

\textbf{Count:} Total prints = $1 + 2 + 3 + \dots + n$.

\textbf{Short way to add:} There is a well-known trick: pair terms from both ends:
\[
(1 + n) + (2 + (n-1)) + \dots
\]
There are $n/2$ pairs each summing about $n$, so the total is about $\frac{n}{2} \times n = \frac{n^2}{2}$. More simply, the exact formula is $n(n+1)/2$ but the main point is it grows like $n^2$.

\textbf{Small example:} If $n=4$, prints = 1+2+3+4 = 10, which is close to $4^2 =16$ (same growth type).

\textbf{Final answer:} $\boxed{O(n^2)}$.

\hrulefill

\item \textbf{What happens?} Start with $i=n$. Each step you replace $i$ by $i$ divided by 2. You stop when $i$ becomes 0.

\textbf{How many steps?} Each step halves the value. So how many halvings until $n$ becomes 0? About the number of times you can divide by 2 until you reach 1 (and one more to reach 0). That number is roughly the logarithm of $n$ (base 2).

\textbf{Small example:} If $n=16$ the sequence is 16,8,4,2,1,0 — about 5 steps, and $\log_2(16)=4$ (plus the final step to 0), so it's logarithmic.

\textbf{Final answer:} $\boxed{O(\log n)}$.

\hrulefill

\item \textbf{What happens?} This function calls itself twice with the input reduced by 1 each time. That means each call spawns two more calls, then each of those spawns two more, and so on.

\textbf{Why that increase rapidly:} The number of calls doubles at each level. If you start at size $n$, after 1 level you have 2 calls, after 2 levels you have 4 calls, after $k$ levels you have $2^k$ calls. Since you need about $n$ levels to reach the base case (reduce to 1), you end up with about $2^{n}$ calls in total.

\textbf{Small example:} For $n=3$:
\begin{itemize}
    \item $f(3)$ calls $f(2)$ and $f(2)$
    \item each $f(2)$ calls $f(1)$ and $f(1)$
    \item $f(1)$ returns immediately
\end{itemize}
Counting calls: $f(3) = 1$, two $f(2) = 2$, four $f(1) = 4$ $\rightarrow$ total 7 calls, which is close to $2^3 - 1 = 7$.


\textbf{Final answer:} This is exponential time, $\boxed{O(2^n)}$.

\hrulefill

\item \textbf{What happens?} For each $i$ from 1 to $n$ you run the inner loop about $\sqrt{i}$ times (because \texttt{int(sqrt(i))} is the limit).

\textbf{How to think about the sum:} Total prints = (approximately) $\sqrt{1} + \sqrt{2} + \dots + \sqrt{n}$.

\textbf{Rough bound idea:}
\begin{itemize}
    \item For the last half of the numbers, say from $n/2$ to $n$, each $\sqrt{i}$ is at least $\sqrt{n/2}$.
    \item There are about $n/2$ numbers in that range, so their contribution alone is about $(n/2) \times \sqrt{n/2}$, which is a constant times $n^{3/2}$.
\end{itemize}
This shows the total behaves like $n^{1.5}$ (written as $n^{3/2}$).

\textbf{Small example:} If $n=9$ then inner counts are (1,1,1,2,2,2,2,2,3) summing to 16. Compare with $9^{1.5} = 27$ — same growth trend for larger $n$.

\textbf{Final answer:} $\boxed{O(n^{3/2})}$ (also written $O(n^{1.5})$).

\hrulefill

\item \textbf{What happens?} Farmer John checks every unordered pair of cows and compares IDs digit-by-digit. Number of pairs and cost per pair both matter.

\textbf{How many pairs?} The number of pairs $(i,j)$ with $i<j$ is roughly $n(n-1)/2$, which behaves like $n^2/2$. So around $n^2$ pairs.

\textbf{How long per pair?} Comparing two IDs digit-by-digit takes time proportional to the number of digits per ID. The problem says each ID has up to about $\log n$ digits. (That notation means the number of digits grows slowly with $n$ — for example, if IDs are numbers up to $n$, digits are about log base 10 of $n$.)

\textbf{Multiply:} Total time $\approx$ number of pairs $\times$ time per pair $\approx n^2 \times \log n$.

\textbf{Final answer:} $\boxed{O(n^2 \log n)}$.

\hrulefill

\item \textbf{What happens?} For each integer $k$ from 1 to $n$, Farmer John writes down the multiples of $k$ up to $n$: that is $k, 2k, 3k, \dots$. For $k$ fixed, how many multiples are there? About $n/k$ of them (because you can multiply $k$ by roughly $n/k$ to get up to $n$).

\textbf{Add for all $k$:} Total listings = sum over $k$ of (about $n/k$). So:
\[
\text{total} \approx n \times \left(1 + \frac{1}{2} + \frac{1}{3} + \dots + \frac{1}{n}\right).
\]
The part in parentheses is the harmonic sum. It grows slowly; you can think of it like the number of times you can repeatedly double (it behaves like the logarithm). A simple intuitive fact: $1 + 1/2 + 1/3 + \dots + 1/n$ grows about like $\ln(n)$ (logarithm).

\textbf{Conclusion:} Total $\approx n \times \ln(n)$, written as $\boxed{O(n \log n)}$.

\textbf{Extra intuition (doubling groups):}
\begin{itemize}
    \item There are $n$ numbers that are multiples of 1.
    \item About $n/2$ numbers that are multiples of 2.
    \item About $n/3$ multiples of 3, and so on.
    \item The sum $n(1 + 1/2 + 1/3 + \dots)$ piles up but not as fast as $n^2$ — it's only $n$ times a slowly growing quantity.
\end{itemize}

\hrulefill

\item \textbf{What happens?} The function does two things:
\begin{enumerate}
    \item it does a loop that performs about $n$ constant-time steps (the \texttt{for} loop), and then
    \item it calls itself twice with half the size (that is, on $n/2$ and $n/2$).
\end{enumerate}

So the running time $T(n)$ satisfies the relation
\[
T(n) \approx 2 \cdot T(n/2) + n.
\]
You can think of this as a tree of recursive calls:
\begin{itemize}
    \item At the top level we do $n$ work.
    \item At the next level we have 2 calls, each doing about $n/2$ work inside their own loop, so the total at that level is $2 \times (n/2) = n$ work.
    \item At the next level we have 4 calls doing $n/4$ each, total again $n$.
\end{itemize}
This continues for about $\log_2 n$ levels (because we keep halving until the size becomes 1). Each level costs about $n$, and there are about $\log n$ levels.

\textbf{Multiply:} $n$ work per level $\times$ about $\log n$ levels = $\boxed{O(n \log n)}$ total.

\textbf{Small example:} If $n=8$:
\begin{itemize}
    \item Level 0: one call does 8 work.
    \item Level 1: two calls each do 4 work $\rightarrow$ total 8.
    \item Level 2: four calls each do 2 work $\rightarrow$ total 8.
    \item Level 3: eight calls each do 1 work $\rightarrow$ total 8.
\end{itemize}
There are 4 levels ($\log_2(8)=3$ plus base), each level 8 work $\rightarrow$ about $8 \times 3$ or $8 \times 4$ depending on exact stopping — bottom line: proportional to $n \log n$.

\textbf{} $\boxed{O(n \log n)}$.

\end{enumerate}

\bigskip
\end{document}